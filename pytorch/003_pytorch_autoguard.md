# PyTorch Autograd

PyTorchì˜ ìë™ ë¯¸ë¶„ ê¸°ëŠ¥ì¸ **Autograd**ëŠ” ëª¨ë¸ í•™ìŠµ ì‹œ ì—­ì „íŒŒ(Backpropagation)ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ê²Œ í•´ì£¼ëŠ” í•µì‹¬ ê¸°ëŠ¥ì…ë‹ˆë‹¤.

---

## 1. ê°œìš” (Overview)

**Autograd**ëŠ” í…ì„œì˜ ì—°ì‚° ì´ë ¥ì„ ê¸°ë¡í•œ **ë™ì  ê³„ì‚° ê·¸ë˜í”„**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ë¯¸ë¶„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ë˜í•œ GPUë¥¼ í™œìš©í•˜ë©´ ëŒ€ê·œëª¨ ì—°ì‚°ì„ í›¨ì”¬ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 2. Autograd ê¸°ë³¸ ê°œë…

- `requires_grad=True`ë¡œ ì„¤ì •ëœ TensorëŠ” ì—°ì‚° ì‹œ **ê¸°ìš¸ê¸° ì¶”ì **ì´ í™œì„±í™”ë©ë‹ˆë‹¤.
- ì—°ì‚° ê²°ê³¼ëŠ” **ì—°ì‚° ê·¸ë˜í”„**ë¡œ êµ¬ì„±ë˜ë©°, `.backward()`ë¥¼ í˜¸ì¶œí•˜ë©´ ìë™ìœ¼ë¡œ ë¯¸ë¶„ì´ ìˆ˜í–‰ë©ë‹ˆë‹¤.
- `.grad` ì†ì„±ìœ¼ë¡œ ê¸°ìš¸ê¸°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 3. ê¸°ë³¸ ì˜ˆì œ

```python
python
ë³µì‚¬í¸ì§‘
import torch

x = torch.ones(2, 2, requires_grad=True)
y = x + 2
z = y * y * 3
out = z.mean()

out.backward()
print(x.grad)

```

---

## 4. ì—­ì „íŒŒ(Backpropagation) ì›ë¦¬

```python
python
ë³µì‚¬í¸ì§‘
x = torch.randn(3, requires_grad=True)
y = x * 2
v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float)
y.backward(v)
print(x.grad)

```

---

## 5. ê¸°ìš¸ê¸° ì¶”ì  ì¤‘ì§€

```python
python
ë³µì‚¬í¸ì§‘
x = torch.ones(2, 2, requires_grad=True)
with torch.no_grad():
    y = x * 2
print(y.requires_grad)  # False

```

---

## 6. ê¸°ìš¸ê¸° ì´ˆê¸°í™”

```python
python
ë³µì‚¬í¸ì§‘
model.zero_grad()
optimizer.zero_grad()

```

---

## 7. ì‹¤ì œ ì‚¬ìš© ì˜ˆ: ì„ í˜• íšŒê·€ (ê¸°ë³¸ ë²„ì „)

```python
python
ë³µì‚¬í¸ì§‘
x = torch.randn(10, 3)
y = torch.randn(10, 1)

w = torch.randn(3, 1, requires_grad=True)
b = torch.randn(1, requires_grad=True)

for _ in range(100):
    pred = x @ w + b
    loss = (pred - y).pow(2).mean()
    loss.backward()

    with torch.no_grad():
        w -= 0.01 * w.grad
        b -= 0.01 * b.grad
        w.grad.zero_()
        b.grad.zero_()

```

---

## 8. Autograd ì£¼ì˜ì‚¬í•­

- `.backward()`ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ì œê±°í•¨ (`retain_graph=True` ì‚¬ìš© ì‹œ ë³´ì¡´ ê°€ëŠ¥)
- `.grad`ëŠ” ëˆ„ì ë˜ë¯€ë¡œ `zero_()` í•„ìš”
- `with torch.no_grad()` ë˜ëŠ” `torch.no_grad()`ëŠ” ì¶”ë¡  ì‹œ ì‚¬ìš©

---

## 9. Autogradì™€ GPU ì—°ì‚° ê²°í•©

CUDAë¥¼ ì‚¬ìš©í•´ GPUì—ì„œ Autograd ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë ¤ë©´ Tensorì™€ ëª¨ë¸ì„ `.to(device)`ë¡œ ì „ì†¡í•´ì•¼ í•©ë‹ˆë‹¤.

```python
python
ë³µì‚¬í¸ì§‘
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì…ë ¥ ë°ì´í„° ë° íŒŒë¼ë¯¸í„°ë¥¼ GPUë¡œ ì´ë™
x = torch.randn(100, 3, device=device)
y = torch.randn(100, 1, device=device)

w = torch.randn(3, 1, requires_grad=True, device=device)
b = torch.randn(1, requires_grad=True, device=device)

# í•™ìŠµ ë£¨í”„
for _ in range(100):
    pred = x @ w + b
    loss = (pred - y).pow(2).mean()
    loss.backward()

    with torch.no_grad():
        w -= 0.01 * w.grad
        b -= 0.01 * b.grad
        w.grad.zero_()
        b.grad.zero_()

# ê²°ê³¼ë¥¼ CPUë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
print(w.cpu().detach())

```

ğŸ’¡ **Tip**: GPUë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ê²½ìš° ì†ë„ í–¥ìƒì´ ë§¤ìš° í¬ë©°, íŠ¹íˆ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹/ëª¨ë¸ì¼ìˆ˜ë¡ íš¨ê³¼ì ì…ë‹ˆë‹¤.

---

## ì°¸ê³  ìë£Œ

- PyTorch Autograd ê³µì‹ ë¬¸ì„œ
- PyTorch CUDA ê³µì‹ ë¬¸ì„œ
- PyTorch íŠœí† ë¦¬ì–¼ - Autograd ê¸°ì´ˆ

---

## History

ì‘ì„±ì¼: `2025-06-30`