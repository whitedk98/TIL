## 1. 개요 (Overview)

양자화(Quantization)는 모델의 크기를 줄이면서도 성능을 최대한 유지하는 딥러닝 모델 최적화 기법입니다. 이 기술은 파라미터와 연산의 정밀도를 낮춰 **모델을 더 작고 빠르게 만들며**, 다양한 환경에서 **더 적은 자원으로 실행할 수 있게 합니다**. 특히 모바일, 임베디드 시스템, 엣지 디바이스 등 **제한된 자원 환경**에서 매우 유용하게 사용됩니다.

---

## 2. 양자화란?

- 양자화는 모델의 파라미터 및 데이터 타입을 **부동소수점에서 정수형 등으로 변환**하여 메모리 및 연산 효율성을 향상시키는 기술입니다.
- 이를 통해 **모델의 용량을 줄이고**, **연산 속도를 향상시키며**, **에너지 효율도 개선**할 수 있습니다.

---

## 3. 모델 크기 계산

```
모델 크기 = 파라미터 수 × 데이터 타입 정밀도
```

예를 들어, BLOOM과 같이 1,760억 개의 파라미터를 가진 모델이 있을 경우:

- **float32 사용 시**: 176B × 4 bytes = 704GB
- **float16 사용 시**: 176B × 2 bytes = 352GB
- **int8 사용 시**: 176B × 1 byte = 176GB

> 데이터 타입의 변경만으로도 모델 크기를 4배 이상 축소할 수 있습니다.
> 

---

## 4. 양자화가 필요한 이유

1. **효율성 향상**: 정수 연산은 부동소수점 연산보다 연산 속도가 빠르며 하드웨어에 친화적입니다.
2. **메모리 절약**: 더 적은 비트 수로 가중치를 표현하므로 전체 모델 크기를 줄일 수 있습니다.
3. **에너지 소비 감소**: 더 적은 연산으로 동일한 추론 결과를 얻을 수 있으므로, 전력 소모가 줄어듭니다.
4. **배포 용이성**: 스마트폰, IoT, 엣지 디바이스 등 리소스 제한 환경에서도 실행 가능성이 커집니다.

---

## 5. 주요 양자화 방식

### ▸ 5.1. 학습 후 정적 양자화 (PTQ, Post-Training Quantization)

### 정의

학습이 완료된 모델에 대해 **가중치와 활성화(activation)** 를 정적으로 양자화하는 방식입니다. 훈련 후 별도의 재학습 없이 양자화를 수행하며, 일부 보정 데이터가 필요합니다.

### 특징

- **간단하고 빠른 구현**: 기존 모델을 그대로 사용하고 양자화만 적용하면 됩니다.
- **보정 데이터 필요**: 모델이 처리할 입력 분포에 맞춰 활성화 범위를 계산하기 위해 calibration data가 필요합니다.
- **성능 저하 가능성**: 정밀도가 낮아져 연산 손실이 발생할 수 있습니다.

### 장점

- 구현이 간단하여 빠르게 적용 가능
- 모델 크기 대폭 축소
- 실행 속도 향상 가능

### 단점

- 활성화 값이 작은 경우 성능 저하가 뚜렷하게 발생할 수 있음
- 보정 데이터 품질이 낮으면 양자화 효과가 떨어질 수 있음

### 예시

- 가중치와 활성화 값을 32bit float → 8bit int로 변환
- 예: `float32` weight → `int8` weight

  
### 프레임워크별 PTQ 처리 지원 및 하드웨어 제약

| 프레임워크 | PTQ 지원 | GPU 지원 | 특징 |
| --- | --- | --- | --- |
| **PyTorch** | 지원 | ❌ (CPU only) | `torch.quantization.convert()`는 CPU 백엔드만 지원하며, GPU 실행을 위한 int8 연산 커널이 제공되지 않음. |
| **TensorFlow (TFLite)** | 지원 | ✅ | `tflite_convert`를 통해 GPU delegate와 함께 실행 가능. GPU에서 INT8 연산을 위한 delegate 최적화 제공. |
| **ONNX Runtime** | 지원 | ✅ (제한적) | `quantize_static` 사용 시 일부 GPU 실행 가능하나 대부분은 CPU backend에 의존함. |
| **OpenVINO** | 지원 | ✅ | Intel GPU, VPU를 포함한 다양한 디바이스에서 정적 양자화 최적화 지원. |
| **TensorRT** | 지원 | ✅ | NVIDIA GPU 기반 inference 최적화 가능. 단, calibration 과정이 반드시 필요함. |

> PyTorch의 PTQ는 현재까지 CPU 전용 백엔드를 중심으로 설계되어 있으므로, GPU 기반 환경에서 실행 성능을 극대화하려면 QAT(Quantization-Aware Training) 또는 TensorRT, TFLite와 같은 엔진 사용이 필요합니다.


---

### ▸ 5.2. 동적 양자화 (Dynamic Quantization)

### 정의

**가중치만 사전에 양자화**하고, **활성화 값은 실행 시에 동적으로 양자화**하여 처리하는 방식입니다.

### 특징

- 가중치는 고정된 `int8`로 저장됨
- 활성화는 실행 중 입력에 따라 실시간으로 계산
- 보정 데이터 불필요

### 장점

- 구현이 간단하고 실시간 입력에 유연
- 훈련 없이 바로 적용 가능

### 단점

- PTQ보다 실행 속도가 다소 느릴 수 있음
- 하드웨어 최적화가 되지 않으면 효과가 제한적일 수 있음

### 사용 사례

- CPU 기반 시스템에서 유용
- 실시간 데이터 처리 및 다양한 입력 처리에 강점을 가짐

---

### ▸ 5.3. 양자화 인식 학습 (QAT, Quantization-Aware Training)

### 정의

모델을 학습할 때부터 **양자화를 고려한 연산을 시뮬레이션하며 훈련**하는 방식입니다.

### 특징

- 훈련 시점부터 정밀도 손실을 모델이 인지하고 학습
- 학습 중에 양자화 오차를 보정
- 양자화 후에도 원래 모델 성능에 매우 가깝게 유지

### 장점

- 성능 저하가 거의 없음
- 특정 하드웨어에 맞춰 최적화 가능
- 정확도 손실이 가장 적은 방식

### 단점

- 구현이 복잡하고 학습 시간이 길어짐
- 양자화에 민감한 layer는 사전 조정이 필요

### 사용 사례

- **자율주행**, **고속 이미지 처리**, **AI 엣지 디바이스** 등 고정밀이 필요한 환경
- 성능 저하 없이 양자화를 적용해야 하는 경우

---

## 6. 양자화 방식 비교 요약

| 방식 | 설명 | 장점 | 단점 | 사용 예시 |
| --- | --- | --- | --- | --- |
| **PTQ** | 학습 후 가중치 & 활성화 정적 양자화 | 빠르고 구현 간단 | 보정 데이터 필요, 성능 저하 가능 | 빠른 배포, 모바일 디바이스 |
| **Dynamic** | 가중치는 양자화, 활성화는 실행 중 동적 처리 | 유연한 처리 | 속도 약간 느림 | CPU 기반 시스템 |
| **QAT** | 학습 중 양자화 인식 | 정확도 유지 | 학습 복잡도 높음 | 자율주행, 엣지 디바이스 |

---

## 7. 성능 및 모델 크기 변화

### ▸ 모델 크기 변화

- float32 → int8 전환 시 **4배까지 크기 축소**
- 예: BLOOM 모델 → 704GB → 176GB

### ▸ 성능 비교

- PTQ: 간단하지만 일부 성능 손실 발생 가능
- Dynamic: 유연하지만 성능은 환경에 따라 다름
- QAT: 가장 우수한 정확도 유지

---

## 8. 양자화의 과제

1. **성능 저하**: 정밀도 손실은 피할 수 없는 부분이며, 미세한 예측이 중요한 작업에서는 영향이 큼
2. **전략 선택**: 모델 구조와 사용 환경에 맞는 양자화 전략 선택이 필수
3. **하드웨어 의존성**: 일부 방식은 GPU/TPU 등 하드웨어의 연산 지원 여부에 따라 성능이 제한될 수 있음

---

## History

작성일: `2025-06-15`