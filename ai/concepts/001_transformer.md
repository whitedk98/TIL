## Transformer 개념 통합 문서

---

## 1. 개요 (Overview)

Transformer는 Attention 메커니즘을 중심으로 설계된 신경망 구조로, 입력 시퀀스의 각 요소 간 관계를 모두 동시에 고려할 수 있도록 설계되었습니다. 기존의 순차적 처리 기반 모델들이 갖는 병렬화의 어려움, 장기 의존성 학습의 한계, 지역 정보 편향 등의 문제를 극복하고자 고안되었으며, 자연어 처리(NLP)를 넘어 컴퓨터 비전(CV), 음성 인식, 시계열 분석 등 다양한 영역으로 확장되고 있습니다.

Transformer는 특히 "Self-Attention"이라는 메커니즘을 통해 각 입력 요소가 다른 모든 요소와 어떻게 연관되는지를 계산하여, 문맥을 더 정확하게 이해할 수 있도록 돕습니다. 또한, GPU 병렬 처리를 극대화할 수 있는 구조적 이점으로 인해 학습과 추론 속도에서 우수한 효율성을 보여줍니다.

---

## 2. Transformer란?

- 2017년 논문 "Attention is All You Need"에서 제안된 모델로, 복잡한 RNN 계열 구조 없이도 높은 성능을 보여줍니다.
- 입력 순서를 강제하지 않고, 전역 문맥을 고려한 Attention 연산을 통해 의미 기반 연산을 수행합니다.
- 자연어 처리, 번역, 문서 요약 뿐 아니라 최근에는 이미지 인식, 음성 분석, 시계열 예측 등으로 범위가 확장되고 있습니다.

---

## 3. 주요 구성 원리

### ▸ Self-Attention

- 입력의 각 토큰이 다른 모든 토큰과 얼마나 관련이 있는지를 계산해 가중치를 부여합니다.
- 문장의 특정 단어가 다른 단어와 어떻게 연결되는지 학습함으로써 문맥 정보를 강화합니다.

### ▸ Positional Encoding

- Transformer는 입력 시퀀스를 병렬로 처리하기 때문에, 순서 정보를 직접적으로 반영하지 않습니다.
- 이를 해결하기 위해 각 입력 토큰에 위치 정보를 담은 **Positional Encoding**을 더합니다.

- Transformer의 구조상 입력 순서 정보가 손실되기 때문에, 이를 보완하는 것이 Positional Encoding입니다. 대표적으로 두 가지 방식이 있습니다:

1. **Sinusoidal Encoding (정적)**
    
    논문에서는 아래와 같이 정의된 사인/코사인 함수를 사용합니다:
    
    $$PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right)$$
    
    - 위치마다 주기성이 다른 고유 벡터가 생성되어 토큰에 더해짐
    - 외삽(extrapolation)에 유리함
2. **Learnable Positional Embedding (학습형)**
    
    GPT 등에서는 위치 벡터 자체를 학습합니다.
    
    - 유연한 위치 표현 가능
    - 단점은 훈련되지 않은 길이에 대해 일반화가 어려움
3. **LLM에서의 확장 기법**
    - **RoPE** (LLaMA): 회전 기반의 위치 인코딩
    - **ALiBi**: 길이 제한 없이 직선적인 attention decay 적용
    - **T5**: 토큰 위치가 아닌 **상대적 거리(Relative Position)** 기반 attention 사용

    ### 사용 방식:
    $$ PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right) $$

- 이를 통해 단어 임베딩만으로는 알 수 없는 **위치 기반 의미 차이**를 모델이 인식할 수 있게 됩니다.

### ▸ 병렬 처리

- 기존의 순환신경망은 데이터를 순차적으로 처리해 병렬 연산이 어려웠지만, Transformer는 입력 데이터를 한 번에 병렬로 처리할 수 있도록 설계되어 GPU나 TPU와 같은 병렬 연산 장비를 효율적으로 활용할 수 있습니다.

### ▸ 주요 방식

1. **Sinusoidal Encoding (고정)**: 사인/코사인 함수를 이용해 위치별로 고유한 벡터 생성
2. **Learnable Positional Embedding**: 위치 임베딩 자체를 학습하는 방식 (GPT 등)
3. **LLM 확장 기법**: RoPE, ALiBi, Relative Position Encoding 등 다양한 응용 기법이 발전함

---

## 4. Transformer 구조의 세부 구성

### ▸ Encoder

- 여러 층의 Self-Attention과 Feed Forward Network로 구성
- 입력 문장의 특징을 추상화하고 표현 벡터로 변환

### ▸ Decoder

- 이전에 생성한 출력을 바탕으로 다음 토큰을 예측
- Masked Self-Attention으로 미래 정보 접근 차단
- Encoder의 출력을 참고하여 최종 출력 생성

### ▸ Multi-head Attention

- 여러 개의 Attention을 병렬로 실행하여 서로 다른 관점의 문맥 정보를 통합

---

## 5. Attention 원리의 수학적 의미

### ▸ Q, K, V의 의미

| 구성 요소 | 설명 | 비유 |
| --- | --- | --- |
| Query (Q) | 내가 궁금한 것 | 질문 |
| Key (K) | 어떤 정보를 찾을 때의 기준 | 책 제목 |
| Value (V) | 실제 정보 | 책의 본문 |

### ▸ 수식

- Query와 Key의 내적을 통해 유사도 산출
- Softmax를 통해 확률화
- 해당 확률을 기반으로 Value를 가중합하여 최종 결과 생성

---

## 6. 기존 CNN / RNN / LSTM과의 비교

| 항목 | CNN | RNN / LSTM | Transformer |
| --- | --- | --- | --- |
| 구조 | 지역 정보 위주 | 순차 처리 | 전역 Attention |
| 병렬성 | 가능 | 불가능 | 매우 우수 |
| 장기 의존성 | 제한적 | 일부 보완 | 뛰어남 |
| 학습 속도 | 빠름 | 느림 | 빠름 |
| 전역 문맥 | 취약 | 제한적 | 강함 |

### ▸ 논리적 차이 정리

- CNN은 국소 정보를 계층적으로 확장하여 특징을 추출하지만, 멀리 떨어진 정보의 연결에는 약합니다.
- RNN은 순차적인 흐름을 잘 반영하지만, 연산 병렬화가 불가능하고 오래된 정보에 대한 학습이 어려워집니다.
- Transformer는 모든 입력을 한 번에 바라보며 각 토큰 간의 관계를 명시적으로 학습하기 때문에 장기 의존성과 전역 문맥에서 압도적 우위를 가집니다.

---

## 7. 기존 모델 대체 가능성과 실제 적용

### ▸ NLP 분야

- BERT, GPT 계열 모델이 기존 LSTM 기반 언어 모델을 완전히 대체
- 번역, 질의응답, 문서 생성, 분류 등 모든 주요 NLP 태스크에서 활용

### ▸ 비전 분야

- Vision Transformer(ViT)는 CNN을 대체하거나 보완하며, 특정 조건에서 더 나은 성능

### ▸ 음성/시계열

- 기존 RNN 기반 음성 인식 시스템을 SpeechTransformer 등으로 대체한 사례 증가

---

## 8. 장점과 단점

| 구분 | 장점 | 단점 |
| --- | --- | --- |
| 병렬 처리 | GPU 활용 극대화 | 메모리 사용량 증가 |
| 전역 문맥 이해 | 문장/문서 수준 정보 반영 | 지역 패턴 인식은 상대적으로 미흡 |
| 구조 확장성 | 다양한 Task에 맞는 구조로 변형 가능 | 구조가 복잡하며 튜닝 요소 많음 |
| 성능 | 긴 문장, 긴 시퀀스에서도 우수 | 연산량이 크고 고성능 환경 요구 |

---

## 9. 정리

Transformer는 기존의 CNN, RNN, LSTM 모델이 갖는 구조적, 성능적 한계를 뛰어넘는 혁신적인 구조로, 다양한 입력 간의 관계를 병렬적이고 전역적으로 처리할 수 있다는 점에서 범용 딥러닝 프레임워크로 자리잡고 있습니다. 다양한 분야에서 실제로 기존 모델을 대체하고 있으며, 앞으로도 다양한 변형을 통해 그 활용 영역은 더욱 확장될 것입니다.

---
## History
작성일: `2025-06-03`  
수정일: `2025-06-22`  
수정일: `2025-06-25`  
