# Transformer 개념

---

## 1. 개요 (Overview)

Transformer는 딥러닝 분야에서 혁신적인 구조로 주목받고 있습니다. 이 문서는 Transformer에 대한 개념, 구조, 원리를 정리한 자료입니다.

---

## 2. Transformer란?

- Transformer는 2017년에 제안된 딥러닝 모델로, 특히 자연어 처리(NLP) 분야에서 높은 성능을 보여줍니다. 
- 기존의 순환신경망(RNN)과 달리 순서를 따라 처리하지 않고, "Self-Attention"이라는 특별한 메커니즘을 사용하여 데이터의 각 부분이 서로 어떻게 관련되어 있는지를 병렬적으로 학습합니다.

---

## 3. Transformer의 원리와 주요 특징

### ▸ Self-Attention

- Self-Attention이란 입력 데이터 내 각 요소가 다른 요소와 얼마나 관련되어 있는지를 계산하여, 중요한 정보에 더 많은 주의를 기울이도록 합니다. 예를 들어, 문장 내 특정 단어가 다른 단어와 어떤 관계인지 파악해 중요한 정보를 강조합니다.

### ▸ Positional Encoding

- Transformer는 데이터를 병렬적으로 처리하기 때문에, 단어의 순서 정보를 유지할 수 없습니다. 이 문제를 해결하기 위해 Positional Encoding이라는 벡터를 각 입력 데이터에 추가하여, 단어의 위치 정보를 인공적으로 부여합니다.

### ▸ 병렬 처리

- 기존의 순환신경망은 데이터를 순차적으로 처리해 병렬 연산이 어려웠지만, Transformer는 입력 데이터를 한 번에 병렬로 처리할 수 있도록 설계되어 GPU나 TPU와 같은 병렬 연산 장비를 효율적으로 활용할 수 있습니다.

---

## 4. Transformer 구조의 세부 이해

### ▸ Encoder

- 인코더는 입력 데이터를 받아서 데이터의 내부적 관계를 분석합니다. 이는 여러 개의 Self-Attention 층과 피드포워드 신경망(FFNN) 층으로 구성되어 있어 입력 데이터를 다양한 방식으로 변형하고 학습합니다.

### ▸ Decoder

- 디코더는 예측과 생성 작업을 수행합니다. 입력 데이터를 바탕으로 Masked Self-Attention이라는 구조를 이용해 순서대로 데이터를 생성합니다. 이후 인코더의 정보를 참고하여 더 정확한 출력을 생성합니다.

### ▸ Multi-head Attention

- Multi-head Attention은 여러 개의 Attention 메커니즘을 병렬로 작동시켜 데이터의 다양한 측면을 동시에 분석합니다. 이는 다양한 관점에서 정보를 조합하여 보다 정확하고 깊이 있는 데이터 처리를 가능하게 합니다.

---

## 5. Transformer의 Attention 원리와 의미

### ▸ Query (Q), Key (K), Value (V)의 개념적 설명

- **Query(Q)**: 모델이 어떤 정보를 알고 싶어하는지 나타내는 질문 역할을 합니다. 사용자의 질문 또는 일반적으로 모델의 입력 데이터를 의미합니다. 
- **Key(K)**: Query에 대한 정보를 찾기 위해 사용되는 키워드나 인덱스 역할을 합니다. 모델이 가지고 있는 데이터 중 사용자가 입력한 데이터에 대한 답을 도출할 때 사용되는 데이터 입니다.
- **Value(V)**: 실제로 찾아낸 정보 그 자체로, 책의 본문 내용과 같습니다. Key를 통해 검색된 결과물입니다.

### ▸ 데이터 측면의 의미와 비유

- **Query(Q)**: 모델에 특정 정보나 단어에 대해 관심을 갖고 "질문"을 하는 것과 같습니다.
- **Key(K)**:  이 질문에 답할 수 있도록 모델이 참조하는 기준이 되는 키워드 또는 인덱스입니다.
- **Value(V)**: 질문과 키워드를 통해 찾아낸 실제 의미 있는 정보나 답변을 의미합니다.

---

## 6. Transformer의 장점 및 단점

### ▸ 장점

- 긴 데이터에서도 성능이 우수합니다.
- 병렬 처리로 인한 빠른 계산 속도
- 다양한 분야에 활용 가능성 높음

### ▸ 단점

- 높은 연산 비용과 계산 자원 요구
- 결과 해석의 어려움 (블랙박스)
- 초기 데이터 편향의 영향 가능성

---


## History

작성일: `2025-06-03`
